{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalised Linear Models - this is different to a 'general linear model'\n",
    "\n",
    "Linear regression is great for building models where the response variables are linear combinations of the predictor variables, (i.e. a when we expect that a constant change in a predictor variable leads to a constant change in the response). In this case, there's no real upper or lower bounds on the values that the response variables can take. This is appropriate for responses variables with Normal distributions.\n",
    "\n",
    "However, we've already seen some of the limitations of linear regression. Recall that with our American election data, we were trying to model the proportion of a population that votes Republican in an election. We want our predictions here to be bounded between 0 and 1, but if we modelled this with linear regression, we could easily end up with predictions that were less than 0 or greater than 1. We have a different type of relationship between the predictors and the response here. \n",
    "\n",
    "Generalized linear models help us model things more broadly by allowing for response variables that take any type of distribution (rather than just Normal). Even more helpful, rather than requiring that the response variable be linear combinations of the predictor variables, they allow for some *function* of the response variable to have this linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as opt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "First we need to understand maximum likelihood estimation (MLE). It is actually very simple. Maximum likelihood estimate is the most likely estimate of some unknown parameter.\n",
    "\n",
    "For example, if you have have a set a data below for a random sample of human IQ and you believe it comes from a normal distribution with some unknown mean and standard deviation ( $\\mathcal{N}(\\mu, \\sigma)$ ) then we can use optimisation to find which values for the mean and standard deviation maximises the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given that you flip 3 heads and 2 tails, what is the most likley probability that a coin flip lands on heads. 60%. a coin was used in the example on purpose. in this example we dont know anything about the the history of the coin. pertend if the exmplae was instead a button that outputs h or t\n",
    "    - this is the example stephen gave in class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kris\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_mean, true_sd = 100, 15\n",
    "iqs = stats.norm(true_mean, true_sd).rvs(100)\n",
    "\n",
    "sns.distplot(iqs)\n",
    "plt.xlabel('IQ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the top formula uses an upside down U, this is to multiple up all the pieces of the formula after the upside down U. \n",
    "\n",
    "the likleihood you see some data is based on some parameter of that data.\n",
    "\n",
    "likliehood is different from probability. \n",
    "\n",
    "log liklihood is used because computers struggle to multiply decimals. if you multiply two decimals the number gets smaller. very quickly the number gets too small for computers to handle. therefore we use log.\n",
    "\n",
    "multiply a bunch of numbers is the same as adding up the logs of those numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of your all your data can be computed by calculating the cumulative product of the likelihood of each data point...\n",
    "\n",
    "$$ L(x|\\theta) = \\prod L(x_i | \\theta) $$\n",
    "\n",
    "Where theta is your guess of the model's parameter values.\n",
    "\n",
    "NOTE: Since computers cannot compute the product of extremely small numbers we need to rewrite this equation. Hopefully from calculus you remember that the cumlative product of a set of numbers is equal to the sum of set of the log of those numbers.\n",
    "\n",
    "$$ logL(x|\\theta) = \\sum logL(x_i | \\theta) $$\n",
    "\n",
    "The model's likelihood can be computed using the `.pdf(data)` function for likelihoods and `.logpdf(data)` for log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 155IQ: 3.2018043441388044e-05\n",
      "3 100IQ: 1.881292916570103e-05\n"
     ]
    }
   ],
   "source": [
    "# Quick Question: What's more likely, 1 person with an IQ of 155 or 3 people with IQs of 100?\n",
    "print(f'1 155IQ:', stats.norm(true_mean, true_sd).pdf(155))\n",
    "print(f'3 100IQ:', stats.norm(true_mean, true_sd).pdf([100, 100, 100]).prod())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above example works because because we are multipling the 100's together. \n",
    "\n",
    "also the above works because we are using normal distribution. if data was INCREDILBY centralized with a small SD the above exmplae would start to stop working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "4.381759174209318e-191\n",
      "8.07306445371078e-182\n"
     ]
    }
   ],
   "source": [
    "# Unlikely values\n",
    "print(stats.norm(0, 1).pdf(iqs).prod())  # Notice that the value of this is equal to zero.\n",
    "\n",
    "# More likely values\n",
    "print(stats.norm(90, 20).pdf(iqs).prod())\n",
    "\n",
    "# True values\n",
    "print(stats.norm(true_mean, true_sd).pdf(iqs).prod())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note the above difference in liklihood is 10^9, like billions of times for likley.\n",
    "\n",
    "if you do the above example with less data points (try with 4), then the liklihoods of 90,20 are much closer to true_mean and true_sd than when we use lots of data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-527565.5433617814\n",
      "-438.3163024801619\n",
      "-416.98195378067413\n"
     ]
    }
   ],
   "source": [
    "# Unlikely values\n",
    "print(stats.norm(0, 1).logpdf(iqs).sum())\n",
    "\n",
    "# More likely values\n",
    "print(stats.norm(90, 20).logpdf(iqs).sum())\n",
    "\n",
    "# True values\n",
    "print(stats.norm(true_mean, true_sd).logpdf(iqs).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The awesome thing about likehoods is just how little bad values of $\\theta$ reduces the model fit. Notice that in the example above just being 10 off on the mean and 5 off on the standard deviation yielded parameters that were 1 billion times less likely. \n",
    "\n",
    "### Exercise\n",
    "1. Write an optimiser to find the most likely parameters for our data given it came from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0\n",
    "std = 0\n",
    "def optimizer(data, mean, std):\n",
    "  \n",
    "    solution = stats.norm(mean, std).logpdf(data).sum()\n",
    "    if stats.norm(mean+1, std+1).logpdf(data).sum() - stats.norm(mean, std).logpdf(data).sum() >1:\n",
    "        def optimizer(data, mean-1, std-1)\n",
    "    elif\n",
    "        retu\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate\n",
    "def objective_function(params, data):\n",
    "    return 0\n",
    "\n",
    "#mle_mean, mle_sd = opt.fmin(objective_fuction, initial_guess, (extra_args,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "norm_mle() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ee5205a71adc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogpdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#  notice this is negitive.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmle_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmle_sd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_mle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmle_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmle_sd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(func, x0, args, xtol, ftol, maxiter, maxfun, full_output, disp, retall, callback, initial_simplex)\u001b[0m\n\u001b[0;32m    407\u001b[0m             'initial_simplex': initial_simplex}\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_minimize_neldermead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[0mretlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fun'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nfev'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'status'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_minimize_neldermead\u001b[1;34m(func, x0, args, callback, maxiter, maxfev, disp, return_all, initial_simplex, xatol, fatol, adaptive, **unknown_options)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mfsim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfsim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: norm_mle() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "# Since most optimisers minimise functions we will multiple our likelihood by -1\n",
    "def norm_mle(params, data):\n",
    "    mu, sd = params\n",
    "    return -stats.norm(mu, sd).logpdf(data).sum() #  notice this is negitive. \n",
    "\n",
    "mle_mean, mle_sd = opt.fmin(norm_mle, [50, 50], (iqs,))\n",
    "print(mle_mean, mle_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.52599564297141 15.55458771142999\n"
     ]
    }
   ],
   "source": [
    "print(iqs.mean(), iqs.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLMs\n",
    "\n",
    "What does this have to do with GLMs? Well in a generalised linear model we will be trying to find the MLE of the parameters directly. However, with ordinary least squares regression, the model optimises the mean squared error which _should_ give us the expected value of $y$ given $X$. \n",
    "\n",
    "So let's describe a GLM for normal linear regression. \n",
    "\n",
    "$$ y \\sim \\mathcal{N}(Xm, \\sigma) $$\n",
    "\n",
    "- the above sigma is for the residuals\n",
    "- fancy N is normal distribution\n",
    "\n",
    "\n",
    "\n",
    "This says $y$ comes from a normal distribution where the mean is the function $Xm$ and the standard deviation is $\\sigma$.\n",
    "\n",
    "It can also be rewritten with respect to the residuals. Which is the form most optimisers compute the solution for.\n",
    "\n",
    "$$ (y - Xm) = residuals \\sim \\mathcal{N}(0, \\sigma) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 2],\n",
       "        [-5]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create some fake data\n",
    "# Create data with slope -5 and intercept 2\n",
    "m = np.asmatrix([2, -5]).T\n",
    "n = 500\n",
    "\n",
    "x = stats.uniform().rvs((n, 1))\n",
    "x = sm.add_constant(x)\n",
    "y = x @ m + stats.norm(0, 2).rvs((len(x), 1))\n",
    "print(y.shape)\n",
    "\n",
    "# plt.plot(x[:, 1], y, 'o')\n",
    "# plt.plot(x[:, 1], x @ m, '.')\n",
    "# plt.show()\n",
    "\n",
    "# sns.distplot(y - y.mean())\n",
    "# plt.xlabel('Variance Y')\n",
    "# plt.show()\n",
    "\n",
    "# sns.distplot(y - x@m, fit=stats.norm)\n",
    "# plt.xlabel('Residuals')\n",
    "# plt.show()\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execerise\n",
    "1. Let's try to make an optimiser that finds the params for our linear model. (Assume the the residuals are normally distributed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(params, data):\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_mle(params, data):\n",
    "    m, b, sigma = params\n",
    "    \n",
    "    model = x @ m + stats.norm(0, 2).rvs((len(x), 1))\n",
    "    \n",
    "    residuals = \n",
    "    return -stats.norm(0, sd).logpdf(residuals).sum() #  notice this is negitive. \n",
    "\n",
    "mle_mean, mle_sd = opt.fmin(norm_mle, [50, 50], (iqs,))\n",
    "print(mle_mean, mle_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate\n",
    "def glm_objective(params, data):\n",
    "    return 0\n",
    "\n",
    "# mle_m = opt.fmin(glm_objective, initial_guess, (extra_args,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     worst fractal dimension  perimeter error  mean area  worst perimeter  \\\n",
      "0                    0.11890           8.5890     1001.0           184.60   \n",
      "1                    0.08902           3.3980     1326.0           158.80   \n",
      "2                    0.08758           4.5850     1203.0           152.50   \n",
      "3                    0.17300           3.4450      386.1            98.87   \n",
      "4                    0.07678           5.4380     1297.0           152.20   \n",
      "5                    0.12440           2.2170      477.1           103.40   \n",
      "6                    0.08368           3.1800     1040.0           153.20   \n",
      "7                    0.11510           3.8560      577.9           110.60   \n",
      "8                    0.10720           2.4060      519.8           106.20   \n",
      "9                    0.20750           2.0390      475.9            97.65   \n",
      "10                   0.08452           2.4660      797.8           123.80   \n",
      "11                   0.10480           3.5640      781.0           136.50   \n",
      "12                   0.10230          11.0700     1123.0           151.70   \n",
      "13                   0.06287           2.9030      782.7           112.00   \n",
      "14                   0.14310           2.0610      578.3           108.80   \n",
      "15                   0.13410           2.8790      658.8           124.10   \n",
      "16                   0.08216           3.1950      684.5           123.40   \n",
      "17                   0.11420           3.8540      798.8           136.80   \n",
      "18                   0.07615           5.8650     1260.0           186.80   \n",
      "19                   0.07259           2.0580      566.3            99.70   \n",
      "20                   0.08183           1.3830      520.0            96.09   \n",
      "21                   0.07773           1.9090      273.9            65.13   \n",
      "22                   0.09946           3.3840      704.4           125.10   \n",
      "23                   0.07526           4.3030     1404.0           188.00   \n",
      "24                   0.09564           5.4550      904.6           177.00   \n",
      "25                   0.10590           7.2760      912.7           152.40   \n",
      "26                   0.12750           2.1100      644.8           122.40   \n",
      "27                   0.07421           5.6320     1094.0           139.90   \n",
      "28                   0.09876           3.4980      732.4           149.30   \n",
      "29                   0.07919           4.6550      955.1           134.90   \n",
      "..                       ...              ...        ...              ...   \n",
      "539                  0.10660           1.4450      170.4            54.49   \n",
      "540                  0.08134           1.6280      402.9            78.78   \n",
      "541                  0.10230           2.6150      656.4           113.50   \n",
      "542                  0.06956           2.1770      668.6           107.40   \n",
      "543                  0.06443           1.5390      538.4            92.48   \n",
      "544                  0.08492           2.0760      584.8            99.17   \n",
      "545                  0.06953           2.0660      573.2            97.58   \n",
      "546                  0.07399           1.3560      324.9            71.12   \n",
      "547                  0.09479           0.9887      320.8            71.08   \n",
      "548                  0.07920           2.0540      285.7            69.10   \n",
      "549                  0.07626           3.5640      361.6            83.90   \n",
      "550                  0.06592           2.1150      360.5            74.08   \n",
      "551                  0.08032           1.9940      378.4            77.80   \n",
      "552                  0.06484           1.4770      507.9            88.10   \n",
      "553                  0.07393           2.1210      264.0            62.86   \n",
      "554                  0.07242           1.5020      514.3            88.84   \n",
      "555                  0.08283           1.4370      321.4            69.57   \n",
      "556                  0.06742           1.6480      311.7            67.88   \n",
      "557                  0.06969           3.6180      271.3            66.50   \n",
      "558                  0.08004           2.2240      657.1           105.90   \n",
      "559                  0.08732           1.9360      403.5            82.28   \n",
      "560                  0.08321           2.8880      600.4           100.20   \n",
      "561                  0.05905           2.0410      386.0            75.19   \n",
      "562                  0.14090           2.3620      716.9           128.70   \n",
      "563                  0.09873           8.7580     1347.0           179.10   \n",
      "564                  0.07115           7.6730     1479.0           166.10   \n",
      "565                  0.06637           5.2030     1261.0           155.00   \n",
      "566                  0.07820           3.4250      858.1           126.70   \n",
      "567                  0.12400           5.7720     1265.0           184.60   \n",
      "568                  0.07039           2.5480      181.0            59.16   \n",
      "\n",
      "     mean radius  smoothness error  concave points error  worst compactness  \\\n",
      "0         17.990          0.006399              0.015870            0.66560   \n",
      "1         20.570          0.005225              0.013400            0.18660   \n",
      "2         19.690          0.006150              0.020580            0.42450   \n",
      "3         11.420          0.009110              0.018670            0.86630   \n",
      "4         20.290          0.011490              0.018850            0.20500   \n",
      "5         12.450          0.007510              0.011370            0.52490   \n",
      "6         18.250          0.004314              0.010390            0.25760   \n",
      "7         13.710          0.008805              0.014480            0.36820   \n",
      "8         13.000          0.005731              0.012260            0.54010   \n",
      "9         12.460          0.007149              0.014320            1.05800   \n",
      "10        16.020          0.004029              0.007591            0.15510   \n",
      "11        15.780          0.005771              0.012820            0.56090   \n",
      "12        19.170          0.003139              0.040900            0.39030   \n",
      "13        15.850          0.009769              0.019920            0.19240   \n",
      "14        13.730          0.006429              0.016280            0.77250   \n",
      "15        14.540          0.005607              0.010900            0.65770   \n",
      "16        14.680          0.005718              0.011090            0.18710   \n",
      "17        16.130          0.007026              0.012970            0.42330   \n",
      "18        19.810          0.006494              0.015210            0.31500   \n",
      "19        13.540          0.008462              0.013150            0.17730   \n",
      "20        13.080          0.004097              0.006490            0.27760   \n",
      "21         9.504          0.009606              0.014210            0.11480   \n",
      "22        15.340          0.006789              0.022520            0.59540   \n",
      "23        21.160          0.004728              0.010380            0.26000   \n",
      "24        16.650          0.006048              0.011300            0.35780   \n",
      "25        17.140          0.008029              0.023970            0.39490   \n",
      "26        14.580          0.004452              0.013520            0.66430   \n",
      "27        18.610          0.010750              0.019110            0.21170   \n",
      "28        15.300          0.005233              0.010830            0.61100   \n",
      "29        17.570          0.005627              0.013540            0.28120   \n",
      "..           ...               ...                   ...                ...   \n",
      "539        7.691          0.015470              0.013640            0.30640   \n",
      "540       11.540          0.012150              0.014940            0.21180   \n",
      "541       14.470          0.007138              0.011620            0.42020   \n",
      "542       14.740          0.004775              0.012690            0.13760   \n",
      "543       13.210          0.004973              0.009117            0.13810   \n",
      "544       13.870          0.006298              0.009061            0.20370   \n",
      "545       13.620          0.005868              0.009064            0.15170   \n",
      "546       10.320          0.007086              0.005495            0.08842   \n",
      "547       10.260          0.010270              0.010970            0.22460   \n",
      "548        9.683          0.007440              0.009615            0.09546   \n",
      "549       10.820          0.008263              0.005917            0.16330   \n",
      "550       10.860          0.009579              0.000000            0.07348   \n",
      "551       11.130          0.003495              0.010240            0.17820   \n",
      "552       12.770          0.008835              0.009305            0.10640   \n",
      "553        9.333          0.010940              0.012820            0.08298   \n",
      "554       12.880          0.008412              0.007620            0.16200   \n",
      "555       10.290          0.012050              0.017210            0.17100   \n",
      "556       10.160          0.012910              0.007082            0.12000   \n",
      "557        9.423          0.011590              0.000000            0.07158   \n",
      "558       14.590          0.004242              0.016060            0.31710   \n",
      "559       11.510          0.008200              0.012670            0.25170   \n",
      "560       14.050          0.007256              0.016260            0.22640   \n",
      "561       11.200          0.007594              0.000000            0.05494   \n",
      "562       15.220          0.004625              0.016080            0.79170   \n",
      "563       20.920          0.006399              0.026240            0.41860   \n",
      "564       21.560          0.010300              0.024540            0.21130   \n",
      "565       20.130          0.005769              0.016780            0.19220   \n",
      "566       16.600          0.005903              0.015570            0.30940   \n",
      "567       20.600          0.006522              0.016640            0.86810   \n",
      "568        7.760          0.007189              0.000000            0.06444   \n",
      "\n",
      "     radius error  concavity error  \n",
      "0          1.0950         0.053730  \n",
      "1          0.5435         0.018600  \n",
      "2          0.7456         0.038320  \n",
      "3          0.4956         0.056610  \n",
      "4          0.7572         0.056880  \n",
      "5          0.3345         0.036720  \n",
      "6          0.4467         0.022540  \n",
      "7          0.5835         0.024880  \n",
      "8          0.3063         0.035530  \n",
      "9          0.2976         0.077430  \n",
      "10         0.3795         0.011010  \n",
      "11         0.5058         0.027910  \n",
      "12         0.9555         0.088900  \n",
      "13         0.4033         0.050510  \n",
      "14         0.2121         0.055010  \n",
      "15         0.3700         0.047410  \n",
      "16         0.4727         0.019980  \n",
      "17         0.5692         0.031880  \n",
      "18         0.7582         0.033910  \n",
      "19         0.2699         0.023870  \n",
      "20         0.1852         0.016980  \n",
      "21         0.2773         0.019850  \n",
      "22         0.4388         0.064460  \n",
      "23         0.6917         0.017150  \n",
      "24         0.8068         0.027410  \n",
      "25         1.0460         0.037320  \n",
      "26         0.2545         0.026810  \n",
      "27         0.8529         0.050810  \n",
      "28         0.4390         0.035760  \n",
      "29         0.6003         0.034070  \n",
      "..            ...              ...  \n",
      "539        0.2196         0.092520  \n",
      "540        0.2784         0.055530  \n",
      "541        0.2542         0.038290  \n",
      "542        0.3031         0.019470  \n",
      "543        0.2351         0.014980  \n",
      "544        0.2720         0.026150  \n",
      "545        0.3460         0.020210  \n",
      "546        0.2104         0.010120  \n",
      "547        0.1144         0.026130  \n",
      "548        0.2957         0.023370  \n",
      "549        0.5196         0.012770  \n",
      "550        0.3163         0.000000  \n",
      "551        0.2800         0.034450  \n",
      "552        0.2409         0.013280  \n",
      "553        0.3013         0.039960  \n",
      "554        0.2116         0.038980  \n",
      "555        0.2199         0.048040  \n",
      "556        0.2441         0.004174  \n",
      "557        0.5375         0.000000  \n",
      "558        0.2254         0.065780  \n",
      "559        0.2388         0.057380  \n",
      "560        0.3645         0.020710  \n",
      "561        0.3141         0.000000  \n",
      "562        0.2602         0.073590  \n",
      "563        0.9622         0.078450  \n",
      "564        1.1760         0.051980  \n",
      "565        0.7655         0.039500  \n",
      "566        0.4564         0.047300  \n",
      "567        0.7260         0.071170  \n",
      "568        0.3857         0.000000  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[569 rows x 10 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dot product shape mismatch, (569, 10) vs (2, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-47bc0d9b287d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogpdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmle_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglm_objective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmle_sigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmle_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(func, x0, args, xtol, ftol, maxiter, maxfun, full_output, disp, retall, callback, initial_simplex)\u001b[0m\n\u001b[0;32m    407\u001b[0m             'initial_simplex': initial_simplex}\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_minimize_neldermead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[0mretlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fun'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nfev'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'status'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_minimize_neldermead\u001b[1;34m(func, x0, args, callback, maxiter, maxfev, disp, return_all, initial_simplex, xatol, fatol, adaptive, **unknown_options)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mfsim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfsim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-47bc0d9b287d>\u001b[0m in \u001b[0;36mglm_objective\u001b[1;34m(params, data, target)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mm_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mm_i\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mresids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogpdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    894\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__matmul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[1;34m\"\"\" Matrix multiplication using binary `@` operator in Python>=3.5 \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 896\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    876\u001b[0m                 raise ValueError('Dot product shape mismatch, '\n\u001b[0;32m    877\u001b[0m                                  '{l} vs {r}'.format(l=lvals.shape,\n\u001b[1;32m--> 878\u001b[1;33m                                                      r=rvals.shape))\n\u001b[0m\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dot product shape mismatch, (569, 10) vs (2, 1)"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "def glm_objective(params, data, target):\n",
    "    print(data)\n",
    "    m_i, sigma_i = np.asmatrix(params[:-1]).T, params[-1]\n",
    "    y_hat = data @ m_i\n",
    "    resids = target - y_hat\n",
    "    return -stats.norm(0, sigma_i).logpdf(resids).sum()\n",
    "\n",
    "mle_params = opt.fmin(glm_objective, np.array([2, 2, 2]), (x, y)) # the x and y in this example are the data and target in \n",
    "# the objective function. \n",
    "\n",
    "mle_sigma = mle_params[-1]\n",
    "mle_params = mle_params[:-1]\n",
    "\n",
    "# print(mle_params)\n",
    "# print(mle_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model has no bias if residuals are centered on zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statsmodels GLM\n",
    "\n",
    "Statsmodels has a GLM function that allows you to do all of this easily. Precisely how this is done is not exactly the same but the goal is identical, that is, fit the residuals to a particular distribution.\n",
    "\n",
    "For data where the residuals are normally distributed the default values are fine, however whenever the residuals deviate for some other reason then we'll need to modify the 'link functions' and 'family'. Basically all packages will pair the appropriate linker function for each family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM Logistic Regression (_Family = Binomial_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>mean area</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>mean radius</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>radius error</th>\n",
       "      <th>concavity error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.11890</td>\n",
       "      <td>8.589</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>184.60</td>\n",
       "      <td>17.99</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.05373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.08902</td>\n",
       "      <td>3.398</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>158.80</td>\n",
       "      <td>20.57</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.01860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08758</td>\n",
       "      <td>4.585</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>152.50</td>\n",
       "      <td>19.69</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.03832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.17300</td>\n",
       "      <td>3.445</td>\n",
       "      <td>386.1</td>\n",
       "      <td>98.87</td>\n",
       "      <td>11.42</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>0.05661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.07678</td>\n",
       "      <td>5.438</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>152.20</td>\n",
       "      <td>20.29</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.05688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worst fractal dimension  perimeter error  mean area  worst perimeter  \\\n",
       "0                  0.11890            8.589     1001.0           184.60   \n",
       "1                  0.08902            3.398     1326.0           158.80   \n",
       "2                  0.08758            4.585     1203.0           152.50   \n",
       "3                  0.17300            3.445      386.1            98.87   \n",
       "4                  0.07678            5.438     1297.0           152.20   \n",
       "\n",
       "   mean radius  smoothness error  concave points error  worst compactness  \\\n",
       "0        17.99          0.006399               0.01587             0.6656   \n",
       "1        20.57          0.005225               0.01340             0.1866   \n",
       "2        19.69          0.006150               0.02058             0.4245   \n",
       "3        11.42          0.009110               0.01867             0.8663   \n",
       "4        20.29          0.011490               0.01885             0.2050   \n",
       "\n",
       "   radius error  concavity error  \n",
       "0        1.0950          0.05373  \n",
       "1        0.5435          0.01860  \n",
       "2        0.7456          0.03832  \n",
       "3        0.4956          0.05661  \n",
       "4        0.7572          0.05688  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Let's quickly prepare the data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast = load_breast_cancer()\n",
    "x = pd.DataFrame(breast.data, columns=breast.feature_names)\n",
    "x = x.sample(10, axis=1, random_state=1992)\n",
    "y = breast.target  # 0 is benign and 1 is malignant\n",
    "\n",
    "display(x.head())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create and fit a model like we have in other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>y</td>        <th>  No. Observations:  </th>  <td>   569</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   559</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>     9</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -62.554</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Mon, 29 Oct 2018</td> <th>  Deviance:          </th> <td>  125.11</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>15:58:44</td>     <th>  Pearson chi2:      </th>  <td>  251.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>         <td>9</td>        <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>                <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst fractal dimension</th> <td>    0.1501</td> <td>   29.523</td> <td>    0.005</td> <td> 0.996</td> <td>  -57.714</td> <td>   58.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>perimeter error</th>         <td>    2.2390</td> <td>    0.706</td> <td>    3.173</td> <td> 0.002</td> <td>    0.856</td> <td>    3.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean area</th>               <td>   -0.0372</td> <td>    0.006</td> <td>   -5.923</td> <td> 0.000</td> <td>   -0.050</td> <td>   -0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst perimeter</th>         <td>   -0.3457</td> <td>    0.068</td> <td>   -5.107</td> <td> 0.000</td> <td>   -0.478</td> <td>   -0.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean radius</th>             <td>    4.6219</td> <td>    0.653</td> <td>    7.074</td> <td> 0.000</td> <td>    3.341</td> <td>    5.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>smoothness error</th>        <td> -204.2461</td> <td>  119.108</td> <td>   -1.715</td> <td> 0.086</td> <td> -437.694</td> <td>   29.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>concave points error</th>    <td> -165.4732</td> <td>   81.621</td> <td>   -2.027</td> <td> 0.043</td> <td> -325.446</td> <td>   -5.500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst compactness</th>       <td>   -8.9332</td> <td>    4.600</td> <td>   -1.942</td> <td> 0.052</td> <td>  -17.949</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>radius error</th>            <td>  -17.6122</td> <td>    4.761</td> <td>   -3.699</td> <td> 0.000</td> <td>  -26.943</td> <td>   -8.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>concavity error</th>         <td>   49.0546</td> <td>   21.764</td> <td>    2.254</td> <td> 0.024</td> <td>    6.398</td> <td>   91.711</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                  569\n",
       "Model:                            GLM   Df Residuals:                      559\n",
       "Model Family:                Binomial   Df Model:                            9\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -62.554\n",
       "Date:                Mon, 29 Oct 2018   Deviance:                       125.11\n",
       "Time:                        15:58:44   Pearson chi2:                     251.\n",
       "No. Iterations:                     9   Covariance Type:             nonrobust\n",
       "===========================================================================================\n",
       "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------\n",
       "worst fractal dimension     0.1501     29.523      0.005      0.996     -57.714      58.014\n",
       "perimeter error             2.2390      0.706      3.173      0.002       0.856       3.622\n",
       "mean area                  -0.0372      0.006     -5.923      0.000      -0.050      -0.025\n",
       "worst perimeter            -0.3457      0.068     -5.107      0.000      -0.478      -0.213\n",
       "mean radius                 4.6219      0.653      7.074      0.000       3.341       5.902\n",
       "smoothness error         -204.2461    119.108     -1.715      0.086    -437.694      29.202\n",
       "concave points error     -165.4732     81.621     -2.027      0.043    -325.446      -5.500\n",
       "worst compactness          -8.9332      4.600     -1.942      0.052     -17.949       0.083\n",
       "radius error              -17.6122      4.761     -3.699      0.000     -26.943      -8.281\n",
       "concavity error            49.0546     21.764      2.254      0.024       6.398      91.711\n",
       "===========================================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logit_glm = sm.GLM(y, x, family=sm.families.Binomial()).fit() # familys are reffering to the distribution of the residuals\n",
    "# we are doing a logistic regression here by specifying Binomial residuals. \n",
    "\n",
    "display(logit_glm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only really a few things worth taking note of.\n",
    "\n",
    "- GLMs do no return the $R^2$ because there is not always an agreed upon way to do so.\n",
    "- The deviance is an _unstandardised_ way of checking how well a model has fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another example of OPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
